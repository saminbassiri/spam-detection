{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install parsivar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import string\n",
    "from parsivar import Normalizer\n",
    "from parsivar import FindStems, Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report, confusion_matrix,  accuracy_score\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(filename):\n",
    "    with open(filename,'r', encoding='utf-8') as infile:\n",
    "        return [line for line in infile if line != \"\\n\"]\n",
    "spam_training_directory = os.getcwd() + '/emails/spamtraining'\n",
    "ham_training_directory  = os.getcwd() + '/emails/hamtraining'\n",
    "spam_testing_directory = os.getcwd() + '/emails/spamtesting'\n",
    "ham_testing_directory = os.getcwd() + '/emails/hamtesting'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_traning_set = []\n",
    "files = os.listdir(spam_training_directory)\n",
    "i = 0\n",
    "for f_name in files:\n",
    "        maile = words(spam_training_directory + '/' + f_name)\n",
    "        text = \"\"\n",
    "        for line in maile:\n",
    "            text = text + line\n",
    "        i += 1\n",
    "        spam_traning_set.append([text,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_traning_set = []\n",
    "files = os.listdir(ham_training_directory)\n",
    "i = 0\n",
    "for f_name in files:\n",
    "    maile = words(ham_training_directory + '/' + f_name)\n",
    "    text = \"\"\n",
    "    for line in maile:\n",
    "        text = text + line\n",
    "    i += 1\n",
    "    ham_traning_set.append([text,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_testing_set = []\n",
    "files = os.listdir(spam_testing_directory)\n",
    "i = 0\n",
    "for f_name in files:\n",
    "        maile = words(spam_testing_directory + '/' + f_name)\n",
    "        text = \"\"\n",
    "        for line in maile:\n",
    "            text = text + line\n",
    "        i += 1\n",
    "        spam_testing_set.append([text,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_testing_set = []\n",
    "files = os.listdir(ham_testing_directory)\n",
    "i = 0\n",
    "for f_name in files:\n",
    "        maile = words(ham_testing_directory + '/' + f_name)\n",
    "        text = \"\"\n",
    "        for line in maile:\n",
    "            text = text + line\n",
    "        i += 1\n",
    "        ham_testing_set.append([text,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a train and testing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maile</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿\\n  *جدیدترین و قدرتمندترین محصول بزرگ کننده ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿\\n    Ghahve JENSI :: قهوه جنسی\\n \\n \\nدر صور...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿\\n*درج **لینک  در 7000 وبلاگ\\n*\\n    درج لینک...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿\\n*فارکس*\\n \\n \\n*فارکس چیست ؟ * &lt;http://fore...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿\\n-------------------------------------------...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               maile  spam\n",
       "0  ﻿\\n  *جدیدترین و قدرتمندترین محصول بزرگ کننده ...     1\n",
       "1  ﻿\\n    Ghahve JENSI :: قهوه جنسی\\n \\n \\nدر صور...     1\n",
       "2  ﻿\\n*درج **لینک  در 7000 وبلاگ\\n*\\n    درج لینک...     1\n",
       "3  ﻿\\n*فارکس*\\n \\n \\n*فارکس چیست ؟ * <http://fore...     1\n",
       "4  ﻿\\n-------------------------------------------...     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_train = DataFrame(spam_traning_set,columns=['maile','spam'])\n",
    "ham_train = DataFrame(ham_traning_set,columns=['maile','spam'])\n",
    "train_set = pd.concat([spam_train, ham_train], ignore_index=True)\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maile</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿\\n*agar ba farsi khandan moshkel darid\\nbe ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿\\n*agar ba farsi khandan moshkel darid\\nbe ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿\\nلطفا اگر این ایمیل را در اسپم مشاهده میکنید...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿\\nارسال 90درصدی به اینباکس\\nهر ثانيه يک ايميل...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿    عنوان بهترین گرافیست را از آن خود کنید!\\n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               maile  spam\n",
       "0  ﻿\\n*agar ba farsi khandan moshkel darid\\nbe ma...     1\n",
       "1  ﻿\\n*agar ba farsi khandan moshkel darid\\nbe ma...     1\n",
       "2  ﻿\\nلطفا اگر این ایمیل را در اسپم مشاهده میکنید...     1\n",
       "3  ﻿\\nارسال 90درصدی به اینباکس\\nهر ثانيه يک ايميل...     1\n",
       "4  ﻿    عنوان بهترین گرافیست را از آن خود کنید!\\n...     1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_test = DataFrame(spam_testing_set,columns=['maile','spam'])\n",
    "ham_test = DataFrame(ham_testing_set,columns=['maile','spam'])\n",
    "test_set = pd.concat([spam_test, ham_test], ignore_index=True)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maile    0\n",
       "spam     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_missing = train_set.isna()\n",
    "train_df_missing.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maile    0\n",
       "spam     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_missing = test_set.isna()\n",
    "test_df_missing.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             spam\n",
       "count  600.000000\n",
       "mean     0.500000\n",
       "std      0.500417\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      0.500000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             spam\n",
       "count  400.000000\n",
       "mean     0.500000\n",
       "std      0.500626\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      0.500000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pun = list(string.punctuation)\n",
    "english_w_l = list(string.ascii_lowercase)\n",
    "english_w_u = list(string.ascii_uppercase)\n",
    "num = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\",] \n",
    "for i in range(0,len(train_set[\"maile\"])):\n",
    "    for p in pun:\n",
    "        train_set.loc[i,\"maile\"] = train_set.loc[i,\"maile\"].replace(p,\" \")\n",
    "    for w in english_w_l:\n",
    "        train_set.loc[i,\"maile\"] = train_set.loc[i,\"maile\"].replace(w,\" \")\n",
    "    for w in english_w_u:\n",
    "        train_set.loc[i,\"maile\"] = train_set.loc[i,\"maile\"].replace(w,\" \")\n",
    "    for n in num:\n",
    "        train_set.loc[i,\"maile\"] = train_set.loc[i,\"maile\"].replace(n,\" \")\n",
    "\n",
    "for i in range(0,len(test_set[\"maile\"])):\n",
    "    for p in pun:\n",
    "        test_set.loc[i,\"maile\"] = test_set.loc[i,\"maile\"].replace(p,\" \")\n",
    "    for w in english_w_l:\n",
    "        test_set.loc[i,\"maile\"] = test_set.loc[i,\"maile\"].replace(w,\" \")\n",
    "    for w in english_w_u:\n",
    "        test_set.loc[i,\"maile\"] = test_set.loc[i,\"maile\"].replace(w,\" \")\n",
    "    for n in num:\n",
    "        test_set.loc[i,\"maile\"] = test_set.loc[i,\"maile\"].replace(n,\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_normalizer = Normalizer(statistical_space_correction=True)\n",
    "for i in range(0,len(train_set[\"maile\"])):\n",
    "    train_set.loc[i,\"maile\"] = my_normalizer.normalize(train_set.loc[i,\"maile\"])\n",
    "for i in range(0,len(test_set[\"maile\"])):\n",
    "    test_set.loc[i,\"maile\"] = my_normalizer.normalize(test_set.loc[i,\"maile\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemmer and Tokenizer and preprocessor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = FindStems()\n",
    "my_tokenizer = Tokenizer()\n",
    "def prep(txt):\n",
    "    txt = txt.split()\n",
    "    txt = [stemmer.convert_to_stem(t) for t in txt]\n",
    "    txt = \" \".join(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"stop_w_p.txt\",'r', encoding='utf-8') as infile:\n",
    "    stop_word = [line for line in infile if line != \"\\n\"]\n",
    "for i in range (0,len(stop_word)):\n",
    "    stop_word[i] = stop_word[i].replace(\"\\n\",\"\")\n",
    "stop_word = set(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "C:\\Users\\Samin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['آسانی', 'آسیب', 'آمد&آ', 'آمر', 'آه', 'آورد&آور', 'ازا', 'اس', 'اسلامی', 'اطلاعند', 'افزود&افزا', 'الاسف', 'الظاهر', 'امیدوار', 'امیدواری', 'بااین', 'بارهٌ', 'بازی', 'بازیگوش', 'باوجودی', 'برآن', 'بردار', 'برنامه', 'بسته', 'بطوری', 'بل', 'بند', 'بود&باش', 'تازگی', 'تحریم', 'تدریج', 'ترتیب', 'تردید', 'ترند', 'تفاوتند', 'تند', 'توانست&توان', 'تک', 'ثانی', 'جمع', 'جمله', 'جنس', 'حسابی', 'حقیر', 'خسته', 'خواست&خواه', 'خوبی', 'خودبه', 'خودی', 'داد&ده', 'داشت&دار', 'داشت&دارد', 'دام', 'دانست&دان', 'دراین', 'درحالی', 'درشتی', 'درصورتی', 'درعین', 'درپی', 'دسته', 'دشمنی', 'دلخواه', 'دیوانه', 'ذلک', 'راجع', 'رسید&رس', 'رغم', 'رفت&رو', 'رفت&روب', 'رنجید&رنج', 'روزه', 'روشنی', 'ریخت&ریز', 'زد&زن', 'زده', 'زودی', 'ساخت&ساز', 'ساده', 'سادگی', 'سازهاست', 'سالم', 'ساله', 'ست', 'سرعت', 'سه', 'سیاه', 'شاأالله', 'شاهد', 'شد&شد', 'شد&شو', 'شدت', 'شست&شوی', 'شناخت&شناس', 'شوق', 'صندوق', 'طوری', 'علاوه', 'علی', 'عملی', 'فهمید&فهم', 'قد', 'لا', 'م', 'مادام', 'مامان', 'مجموع', 'مذهبی', 'مراتب', 'مع', 'معتقد', 'معذور', 'مقصر', 'موزد', 'ناراضی', 'ناچار', 'نخود', 'نم', 'نهایت', 'نیازمند', 'نیستی', 'هاست', 'هدف', 'هرحال', 'هق', 'و', 'وابسته', 'واقع', 'وجه', 'وحشت', 'وضوح', 'پاره', 'پایید&پا', 'پایین', 'پذیرفت&پذیر', 'پرس', 'پشتوانه', 'پهن', 'چاله', 'چاپلوس', 'چشم', 'کرات', 'کرد&کن', 'کشت&کار', 'کنایه', 'گذاشت&گذار', 'گرفت&گیر', 'گرمی', 'گشت&گرد', 'گفت&گو', 'یافت&یاب', 'یواش'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=5000, min_df=5,\n",
       "                preprocessor=<function prep at 0x000001EE0C3EDEE0>,\n",
       "                stop_words={'آباد', 'آخ', 'آخر', 'آخرها', 'آخه', 'آدمهاست',\n",
       "                            'آرام', 'آرام آرام', 'آره', 'آری', 'آزادانه',\n",
       "                            'آسان', 'آسیب پذیرند', 'آشنایند', 'آشکارا', 'آقا',\n",
       "                            'آقای', 'آقایان', 'آمد', 'آمدن', 'آمده', 'آمرانه',\n",
       "                            'آن', 'آن گاه', 'آنان', 'آنانی', 'آنجا', 'آنرا',\n",
       "                            'آنطور', 'آنقدر', ...},\n",
       "                tokenizer=<bound method Tokenizer.tokenize_words of <parsivar.tokenizer.Tokenizer object at 0x000001EE091E2A30>>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_text = train_set[\"maile\"].to_list()\n",
    "vectorizer = CountVectorizer(stop_words=stop_word, tokenizer= my_tokenizer.tokenize_words ,max_features=5000, ngram_range=(1,1),min_df=5,preprocessor=prep)\n",
    "vectorizer.fit(training_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2203)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = vectorizer.transform(training_text).toarray()\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 2203)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_text = test_set[\"maile\"].to_list()\n",
    "x_test = vectorizer.transform(testing_text).toarray()\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set[\"spam\"].to_list()\n",
    "y_test = test_set[\"spam\"].tolist()\n",
    "transformer = SelectKBest(chi2, k=500).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 500)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = transformer.transform(x_train)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 500)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = transformer.transform(x_test)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosin function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosin_sim(vec_a,vec_b):\n",
    "    dot = sum(a*b for a, b in zip(vec_a, vec_b))\n",
    "    norm_a = sum(a*a for a in vec_a) ** 0.5\n",
    "    norm_b = sum(b*b for b in vec_b) ** 0.5\n",
    "    return dot / (norm_a*norm_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# knn class for cosin_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.data = X\n",
    "        self.targets = y\n",
    "\n",
    "    def cosin_distance(self, x):\n",
    "        dists = []\n",
    "        for ts in x:\n",
    "            d = []\n",
    "            for tr in self.data:\n",
    "                d.append(cosin_sim(ts,tr))\n",
    "            dists.append(np.array(d))\n",
    "        return dists\n",
    "    \n",
    "    def predict_cosin(self, x, k=1):\n",
    "        # compute distance between input and training data\n",
    "        dists = self.cosin_distance(x)\n",
    "        # find the k nearest neighbors and their classifications\n",
    "        result = []\n",
    "        for test_dis in dists:\n",
    "            knn = np.argsort(test_dis)[(-k):]\n",
    "            y_knn = []\n",
    "            for index in knn:\n",
    "                y_knn.append(y_train[index])\n",
    "            result.append(max(y_knn, key=y_knn.count))\n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-5b7b4bf3ccf8>:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return dot / (norm_a*norm_b)\n"
     ]
    }
   ],
   "source": [
    "knn = kNN()\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict_cosin(x_test, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute accuracy and confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = 0\n",
    "true_spam = 0\n",
    "true_ham = 0\n",
    "false_spam = 0\n",
    "false_ham = 0\n",
    "for i in range(0,len(y_pred)):\n",
    "    if(y_test[i] == y_pred[i] & y_pred[i] == 1):\n",
    "        true_spam += 1\n",
    "    else:\n",
    "        if(y_test[i] == y_pred[i]):\n",
    "            true_ham += 1\n",
    "        else:\n",
    "            if(y_pred[i] == 1):\n",
    "                false_spam += 1\n",
    "            else:\n",
    "                false_ham += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.8175\n",
      "\n",
      "true_spam : 132\n",
      "false_spam : 5\n",
      "\n",
      "true_ham : 195\n",
      "false_ham : 68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct_pred = true_spam + true_ham\n",
    "print(\"accuracy : \" + str(correct_pred/len(y_pred)) + \"\\n\")\n",
    "print(\"true_spam : \" + str(true_spam))\n",
    "print(\"false_spam : \" + str(false_spam) + \"\\n\")\n",
    "print(\"true_ham : \" + str(true_ham))\n",
    "print(\"false_ham : \" + str(false_ham) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "confusion_matrix\n",
      "[[195   5]\n",
      " [ 68 132]]\n",
      "\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.97      0.84       200\n",
      "           1       0.96      0.66      0.78       200\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.85      0.82      0.81       400\n",
      "weighted avg       0.85      0.82      0.81       400\n",
      "\n",
      "\n",
      "Accuracy\n",
      "81.75 %\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nconfusion_matrix\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nclassification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy\")\n",
    "print(accuracy_score(y_test, y_pred)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert email to list of uniqe word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_email(email):\n",
    "    txt = my_tokenizer.tokenize_words(email)\n",
    "    vec = [stemmer.convert_to_stem(t) for t in txt if t not in stop_word]\n",
    "    return list(OrderedDict.fromkeys(vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count number of word in an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(list_of_word, email):\n",
    "    e_words = my_tokenizer.tokenize_words(email)\n",
    "    e_words = [stemmer.convert_to_stem(t) for t in e_words if t not in stop_word]\n",
    "    res = [e_words.count(w) for w in list_of_word]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute tf_idf for an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tf_idf(list_train_text, email):\n",
    "    word_of_email = vector_email(email)\n",
    "    tf_idf_vec = []\n",
    "    for train_mail in list_train_text:\n",
    "        tf_idf_vec.append(count_words(word_of_email, train_mail))\n",
    "    df_words = [0] * len(word_of_email)\n",
    "    for email_train_vec in tf_idf_vec:\n",
    "        for i in range(0, len(df_words)):\n",
    "            if(email_train_vec[i] > 0):\n",
    "                df_words[i] += 1\n",
    "    dists = []\n",
    "    n = len(tf_idf_vec)\n",
    "    for email_train_vec in tf_idf_vec:\n",
    "        score_email = 0\n",
    "        for i in range(0,len(email_train_vec)):\n",
    "            if(email_train_vec[i] > 0):\n",
    "                tf = np.log(email_train_vec[i]) + 1\n",
    "            else:\n",
    "                tf = 0\n",
    "            if(df_words[i] > 0):\n",
    "                score_email += tf * (np.log(n / df_words[i]))\n",
    "        dists.append(score_email)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.420235002326969, 1.0986122886681098, 3.2364130910642706]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tf_idf(train_set[\"maile\"][:3],test_set[\"maile\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN_tfidf():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.data = X\n",
    "        self.targets = y\n",
    "\n",
    "    def tf_idf_distance(self, x):\n",
    "        dists = []\n",
    "        for ts in x:\n",
    "            dists.append(np.array(my_tf_idf(self.data, ts)))\n",
    "        return dists\n",
    "    \n",
    "    def predict_tf_idf(self, x, k=1):\n",
    "        # compute distance between input and training data\n",
    "        dists = self.tf_idf_distance(x)\n",
    "        # find the k nearest neighbors and their classifications\n",
    "        result = []\n",
    "        for test_dis in dists:\n",
    "            knn = np.argsort(test_dis)[(-k):]\n",
    "            y_knn = []\n",
    "            for index in knn:\n",
    "                y_knn.append(y_train[index])\n",
    "            result.append(max(y_knn, key=y_knn.count))\n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn =  kNN_tfidf()\n",
    "knn.fit(train_set[\"maile\"], y_train)\n",
    "y_pred = knn.predict_tf_idf(test_set[\"maile\"], k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute accuracy and confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "confusion_matrix\n",
      "[[125  75]\n",
      " [  5 195]]\n",
      "\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.62      0.76       200\n",
      "           1       0.72      0.97      0.83       200\n",
      "\n",
      "    accuracy                           0.80       400\n",
      "   macro avg       0.84      0.80      0.79       400\n",
      "weighted avg       0.84      0.80      0.79       400\n",
      "\n",
      "\n",
      "Accuracy\n",
      "80.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nconfusion_matrix\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nclassification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy\")\n",
    "print(accuracy_score(y_test, y_pred)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# navie bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class navei_bayes():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        words_of_test_mail = []\n",
    "        for test_mail in x:\n",
    "            l = my_tokenizer.tokenize_words(test_mail)\n",
    "            for w in l:\n",
    "                words_of_test_mail.append(stemmer.convert_to_stem(w))\n",
    "        words_of_test_mail = [w for w in words_of_test_mail if w not in stop_word]\n",
    "        self.words = list(OrderedDict.fromkeys(words_of_test_mail))\n",
    "        count_number_of_word_spam = []\n",
    "        count_number_of_word_ham = []\n",
    "        n_spam = 0\n",
    "        n_ham = 0\n",
    "        for i in range(0,len(y)):\n",
    "            if y[i] == 1:\n",
    "                n_spam += 1\n",
    "                count_number_of_word_spam.append(count_words(self.words,x[i]))\n",
    "            else:\n",
    "                n_ham += 1\n",
    "                count_number_of_word_ham.append(count_words(self.words,x[i]))\n",
    "\n",
    "        n_word_spam = [0] * len(self.words)\n",
    "        for l in count_number_of_word_spam:\n",
    "            for i in range(0,len(l)):\n",
    "                n_word_spam[i] += l[i]\n",
    "        n_word_ham = [0] * len(self.words)\n",
    "        for l in count_number_of_word_ham:\n",
    "            for i in range(0,len(l)):\n",
    "                n_word_ham[i] += l[i]\n",
    "        self.sum_spam = sum(n_word_spam)\n",
    "        self.sum_ham = sum(n_word_ham)\n",
    "        self.p_ham = n_spam/(n_spam + n_ham)\n",
    "        self.p_spam = n_spam/(n_spam + n_ham)\n",
    "        self.n_word_spam = [w/self.sum_spam for w in n_word_spam]\n",
    "        self.n_word_ham = [w/self.sum_spam for w in n_word_ham]\n",
    "        \n",
    "    def transform(self, x_test):\n",
    "        result = []\n",
    "        for test_mail in x_test:\n",
    "            mail_spam = self.p_spam\n",
    "            mail_ham = self.p_ham\n",
    "            l = my_tokenizer.tokenize_words(test_mail)\n",
    "            l = [stemmer.convert_to_stem(w) for w in l if w not in stop_word]\n",
    "            for w_test in l:\n",
    "                if w_test in self.words:\n",
    "                    indx = self.words.index(w_test)\n",
    "                    if(self.n_word_spam[indx] > 0):\n",
    "                        mail_spam *= (self.n_word_spam[indx])/((self.p_spam*self.n_word_spam[indx]) + (self.n_word_ham[indx]*self.p_ham))\n",
    "                    if(self.n_word_ham[indx] > 0):\n",
    "                        mail_ham *= (self.n_word_ham[indx])/((self.p_spam*self.n_word_spam[indx]) + (self.n_word_ham[indx]*self.p_ham))\n",
    "            if mail_spam > mail_ham :\n",
    "                result.append(1)\n",
    "            else : \n",
    "                if mail_spam < mail_ham :\n",
    "                    result.append(0)\n",
    "                else :\n",
    "                    result.append(np.nan)\n",
    "        return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_b = navei_bayes()\n",
    "n_b.fit(train_set[\"maile\"],train_set[\"spam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = n_b.transform(test_set[\"maile\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "confusion_matrix\n",
      "[[ 85 115]\n",
      " [  0 200]]\n",
      "\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.42      0.60       200\n",
      "           1       0.63      1.00      0.78       200\n",
      "\n",
      "    accuracy                           0.71       400\n",
      "   macro avg       0.82      0.71      0.69       400\n",
      "weighted avg       0.82      0.71      0.69       400\n",
      "\n",
      "\n",
      "Accuracy\n",
      "71.25 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix,  accuracy_score\n",
    "print(\"\\nconfusion_matrix\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nclassification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy\")\n",
    "print(accuracy_score(y_test, y_pred)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
